{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Retrieval System Comparison\n",
    "\n",
    "**Tesis:** Dise\u00f1o y Validaci\u00f3n de un Modelo Sem\u00e1ntico H\u00edbrido para Optimizar Sistemas RAG\n",
    "\n",
    "Within-subjects experiment comparing 3 retrieval systems:\n",
    "1. **Control 1 - BM25**: Pure lexical retrieval (BM25Okapi)\n",
    "2. **Control 2 - Dense**: Pure semantic retrieval (BGE-large + FAISS)\n",
    "3. **Experimental - Hybrid**: BM25 + Dense with RRF/Linear fusion + optional reranking\n",
    "\n",
    "**Variables:** Fusion method (RRF vs Linear), alpha values, reranker presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Hybrid Index\n",
    "\n",
    "Load the pre-built FAISS + BM25 indices (adaptive/500/bge-large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding.embedding_manager import EmbeddingManager\n",
    "from src.embedding.index.hybrid_index import HybridIndex\n",
    "from src.retrieval.query_processor import QueryProcessor\n",
    "from src.retrieval.bm25_retriever import BM25Retriever\n",
    "from src.retrieval.dense_retriever import DenseRetriever\n",
    "from src.retrieval.hybrid_retriever import HybridRetriever\n",
    "from src.reranking.no_reranker import NoReranker\n",
    "\n",
    "# Initialize components\n",
    "embedding_manager = EmbeddingManager(\n",
    "    model_name='bge-large',\n",
    "    cache_dir='../data/embeddings',\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "hybrid_index = HybridIndex(\n",
    "    embedding_manager=embedding_manager,\n",
    "    bm25_k1=1.2,\n",
    "    bm25_b=0.75,\n",
    "    indices_dir='../data/indices',\n",
    ")\n",
    "\n",
    "# Load pre-built indices\n",
    "hybrid_index.load(chunk_strategy='adaptive', chunk_size=500)\n",
    "stats = hybrid_index.get_stats()\n",
    "\n",
    "print(f\"FAISS: {stats['faiss']['total_vectors']} vectors, dim={stats['faiss']['dimension']}\")\n",
    "print(f\"BM25: {stats['bm25']['num_documents']} documents\")\n",
    "print(f\"Chunk map: {stats['chunk_map_size']} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrievers\n",
    "query_processor = QueryProcessor()\n",
    "\n",
    "bm25_retriever = BM25Retriever(hybrid_index, query_processor)\n",
    "dense_retriever = DenseRetriever(hybrid_index, query_processor)\n",
    "hybrid_retriever = HybridRetriever(\n",
    "    hybrid_index,\n",
    "    query_processor=query_processor,\n",
    "    reranker=None,  # No reranker for baseline comparison\n",
    "    fusion_method='rrf',\n",
    "    alpha=0.5,\n",
    "    rrf_k=60,\n",
    ")\n",
    "\n",
    "print('All 3 retrieval systems ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Queries\n",
    "\n",
    "Compare all 3 systems on single-provider and cross-cloud queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test queries\n",
    "test_queries = [\n",
    "    # Single-provider (AWS-focused)\n",
    "    'How to create a VPC in AWS?',\n",
    "    'Configure an S3 bucket lifecycle policy',\n",
    "    'AWS Lambda function deployment with environment variables',\n",
    "    \n",
    "    # Single-provider (Azure-focused)\n",
    "    'Deploy Azure Functions with HTTP trigger',\n",
    "    'Azure Kubernetes Service AKS cluster setup',\n",
    "    \n",
    "    # Single-provider (GCP-focused)\n",
    "    'Google Cloud Storage bucket permissions',\n",
    "    'GCP Compute Engine instance types and pricing',\n",
    "    \n",
    "    # Cross-cloud queries\n",
    "    'Compare serverless computing options between AWS, Azure and GCP',\n",
    "    'Container orchestration best practices across cloud providers',\n",
    "    'IAM identity and access management policies comparison',\n",
    "    \n",
    "    # Conceptual\n",
    "    'What is a service mesh and how does it work?',\n",
    "    'Kubernetes pod lifecycle and restart policies',\n",
    "]\n",
    "\n",
    "print(f'Total test queries: {len(test_queries)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all 3 systems on each query\n",
    "TOP_K = 5\n",
    "all_results = {}\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f'\\nQuery: \"{query}\"')\n",
    "    print('-' * 60)\n",
    "    \n",
    "    # BM25\n",
    "    start = time.time()\n",
    "    bm25_res = bm25_retriever.search(query, top_k=TOP_K)\n",
    "    bm25_time = time.time() - start\n",
    "    \n",
    "    # Dense\n",
    "    start = time.time()\n",
    "    dense_res = dense_retriever.search(query, top_k=TOP_K)\n",
    "    dense_time = time.time() - start\n",
    "    \n",
    "    # Hybrid (RRF)\n",
    "    start = time.time()\n",
    "    hybrid_res = hybrid_retriever.search(query, top_k=TOP_K, fusion='rrf', use_reranker=False)\n",
    "    hybrid_time = time.time() - start\n",
    "    \n",
    "    all_results[query] = {\n",
    "        'bm25': {'results': bm25_res, 'time': bm25_time},\n",
    "        'dense': {'results': dense_res, 'time': dense_time},\n",
    "        'hybrid_rrf': {'results': hybrid_res, 'time': hybrid_time},\n",
    "    }\n",
    "    \n",
    "    # Summary\n",
    "    for sys_name, data in all_results[query].items():\n",
    "        providers = set(r.cloud_provider for r in data['results'])\n",
    "        print(f'  {sys_name:12s}: {len(data[\"results\"])} results, '\n",
    "              f'providers={providers}, time={data[\"time\"]*1000:.0f}ms')\n",
    "\n",
    "print(f'\\nCompleted {len(test_queries)} queries x 3 systems = {len(test_queries)*3} searches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detailed Result Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(query, results_dict, top_n=5):\n",
    "    \"\"\"Display results from all 3 systems side by side.\"\"\"\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'Query: \"{query}\"')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    for sys_name, data in results_dict.items():\n",
    "        print(f'\\n--- {sys_name.upper()} ({data[\"time\"]*1000:.0f}ms) ---')\n",
    "        for i, r in enumerate(data['results'][:top_n]):\n",
    "            text_preview = r.chunk_text[:120].replace('\\n', ' ')\n",
    "            print(f'  [{i+1}] score={r.score:.4f} | {r.cloud_provider}/{r.service_name} | {r.doc_type}')\n",
    "            print(f'      {text_preview}...')\n",
    "            if r.heading_path:\n",
    "                print(f'      Path: {r.heading_path}')\n",
    "\n",
    "# Show detailed results for key queries\n",
    "show_results('How to create a VPC in AWS?', all_results['How to create a VPC in AWS?'])\n",
    "show_results('Compare serverless computing options between AWS, Azure and GCP',\n",
    "             all_results['Compare serverless computing options between AWS, Azure and GCP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Response Time Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect timing data\n",
    "timing_data = []\n",
    "for query, res_dict in all_results.items():\n",
    "    for sys_name, data in res_dict.items():\n",
    "        timing_data.append({\n",
    "            'Query': query[:40] + '...',\n",
    "            'System': sys_name,\n",
    "            'Time (ms)': data['time'] * 1000,\n",
    "        })\n",
    "\n",
    "df_timing = pd.DataFrame(timing_data)\n",
    "\n",
    "# Summary stats\n",
    "print('Average Response Time by System:')\n",
    "print(df_timing.groupby('System')['Time (ms)'].agg(['mean', 'std', 'min', 'max']).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "system_colors = {'bm25': '#FF9800', 'dense': '#2196F3', 'hybrid_rrf': '#4CAF50'}\n",
    "sns.boxplot(data=df_timing, x='System', y='Time (ms)', palette=system_colors, ax=ax)\n",
    "sns.stripplot(data=df_timing, x='System', y='Time (ms)', color='black', alpha=0.3, ax=ax)\n",
    "\n",
    "ax.set_title('Response Time Distribution by Retrieval System')\n",
    "ax.set_ylabel('Time (milliseconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/retrieval_timing.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Result Overlap Analysis\n",
    "\n",
    "How much do the results from different systems overlap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Jaccard similarity between result sets\n",
    "overlap_data = []\n",
    "\n",
    "for query in test_queries:\n",
    "    res = all_results[query]\n",
    "    \n",
    "    bm25_ids = set(r.chunk_id for r in res['bm25']['results'])\n",
    "    dense_ids = set(r.chunk_id for r in res['dense']['results'])\n",
    "    hybrid_ids = set(r.chunk_id for r in res['hybrid_rrf']['results'])\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    def jaccard(a, b):\n",
    "        if not a and not b:\n",
    "            return 1.0\n",
    "        return len(a & b) / len(a | b) if (a | b) else 0.0\n",
    "    \n",
    "    overlap_data.append({\n",
    "        'Query': query[:40] + '...',\n",
    "        'BM25 \\u2229 Dense': jaccard(bm25_ids, dense_ids),\n",
    "        'BM25 \\u2229 Hybrid': jaccard(bm25_ids, hybrid_ids),\n",
    "        'Dense \\u2229 Hybrid': jaccard(dense_ids, hybrid_ids),\n",
    "        'BM25 unique': len(bm25_ids - dense_ids - hybrid_ids),\n",
    "        'Dense unique': len(dense_ids - bm25_ids - hybrid_ids),\n",
    "        'Hybrid unique': len(hybrid_ids - bm25_ids - dense_ids),\n",
    "    })\n",
    "\n",
    "df_overlap = pd.DataFrame(overlap_data)\n",
    "display(df_overlap.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap heatmap\n",
    "avg_overlap = df_overlap[['BM25 \\u2229 Dense', 'BM25 \\u2229 Hybrid', 'Dense \\u2229 Hybrid']].mean()\n",
    "\n",
    "# Build symmetric matrix\n",
    "systems = ['BM25', 'Dense', 'Hybrid']\n",
    "overlap_matrix = np.ones((3, 3))\n",
    "overlap_matrix[0, 1] = overlap_matrix[1, 0] = avg_overlap['BM25 \\u2229 Dense']\n",
    "overlap_matrix[0, 2] = overlap_matrix[2, 0] = avg_overlap['BM25 \\u2229 Hybrid']\n",
    "overlap_matrix[1, 2] = overlap_matrix[2, 1] = avg_overlap['Dense \\u2229 Hybrid']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(overlap_matrix, index=systems, columns=systems),\n",
    "    annot=True, fmt='.3f', cmap='YlGnBu', vmin=0, vmax=1, ax=ax\n",
    ")\n",
    "ax.set_title('Average Jaccard Overlap Between Retrieval Systems (Top-5)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/retrieval_overlap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Provider Diversity Analysis\n",
    "\n",
    "Which system retrieves from the most diverse set of providers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider diversity per query per system\n",
    "diversity_data = []\n",
    "\n",
    "for query in test_queries:\n",
    "    res = all_results[query]\n",
    "    \n",
    "    for sys_name, data in res.items():\n",
    "        providers = [r.cloud_provider for r in data['results']]\n",
    "        unique_providers = set(providers)\n",
    "        \n",
    "        diversity_data.append({\n",
    "            'Query': query[:40] + '...',\n",
    "            'System': sys_name,\n",
    "            'Unique Providers': len(unique_providers),\n",
    "            'Providers': ', '.join(sorted(unique_providers)),\n",
    "        })\n",
    "\n",
    "df_diversity = pd.DataFrame(diversity_data)\n",
    "\n",
    "# Average diversity\n",
    "print('Average Provider Diversity (unique providers in top-5):')\n",
    "print(df_diversity.groupby('System')['Unique Providers'].agg(['mean', 'std', 'min', 'max']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-cloud queries: check if hybrid gets chunks from all relevant providers\n",
    "cross_cloud_queries = [\n",
    "    'Compare serverless computing options between AWS, Azure and GCP',\n",
    "    'Container orchestration best practices across cloud providers',\n",
    "    'IAM identity and access management policies comparison',\n",
    "]\n",
    "\n",
    "print('Cross-Cloud Query Results - Provider Coverage:')\n",
    "print('=' * 70)\n",
    "\n",
    "for query in cross_cloud_queries:\n",
    "    print(f'\\nQuery: \"{query}\"')\n",
    "    res = all_results[query]\n",
    "    \n",
    "    for sys_name, data in res.items():\n",
    "        providers = [r.cloud_provider for r in data['results']]\n",
    "        provider_counts = {}\n",
    "        for p in providers:\n",
    "            provider_counts[p] = provider_counts.get(p, 0) + 1\n",
    "        print(f'  {sys_name:12s}: {dict(sorted(provider_counts.items()))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fusion Method Comparison\n",
    "\n",
    "Compare Linear vs RRF fusion with different alpha values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search: fusion methods x alpha values\n",
    "test_query_for_grid = 'How to create a VPC in AWS?'\n",
    "\n",
    "grid_results = hybrid_retriever.grid_search_alpha(\n",
    "    query=test_query_for_grid,\n",
    "    alpha_values=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    fusion_methods=['linear', 'rrf'],\n",
    "    top_k=5,\n",
    "    top_k_candidates=50,\n",
    ")\n",
    "\n",
    "print(f'Grid search configs tested: {len(grid_results)}')\n",
    "for key, data in grid_results.items():\n",
    "    ids_preview = [r.chunk_id[:8] for r in data['results']]\n",
    "    providers = [r.cloud_provider for r in data['results']]\n",
    "    print(f'  {key:25s}: providers={set(providers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-query grid search to find best alpha/fusion\n",
    "grid_queries = [\n",
    "    'How to create a VPC in AWS?',\n",
    "    'Compare serverless computing options between AWS, Azure and GCP',\n",
    "    'Kubernetes pod lifecycle and restart policies',\n",
    "    'Configure an S3 bucket lifecycle policy',\n",
    "    'Azure Kubernetes Service AKS cluster setup',\n",
    "]\n",
    "\n",
    "fusion_methods = ['linear', 'rrf']\n",
    "alpha_values = [0.3, 0.5, 0.7]\n",
    "\n",
    "# Track how results differ across configs\n",
    "config_scores = {}\n",
    "\n",
    "for query in grid_queries:\n",
    "    grid_res = hybrid_retriever.grid_search_alpha(\n",
    "        query=query,\n",
    "        alpha_values=alpha_values,\n",
    "        fusion_methods=fusion_methods,\n",
    "        top_k=5,\n",
    "    )\n",
    "    \n",
    "    for key, data in grid_res.items():\n",
    "        if key not in config_scores:\n",
    "            config_scores[key] = {'n_providers': [], 'avg_score': []}\n",
    "        \n",
    "        providers = set(r.cloud_provider for r in data['results'])\n",
    "        avg_score = np.mean([r.score for r in data['results']]) if data['results'] else 0\n",
    "        config_scores[key]['n_providers'].append(len(providers))\n",
    "        config_scores[key]['avg_score'].append(avg_score)\n",
    "\n",
    "# Display summary\n",
    "config_summary = []\n",
    "for key, scores in config_scores.items():\n",
    "    config_summary.append({\n",
    "        'Config': key,\n",
    "        'Avg Providers': np.mean(scores['n_providers']),\n",
    "        'Avg Score': np.mean(scores['avg_score']),\n",
    "    })\n",
    "\n",
    "df_configs = pd.DataFrame(config_summary).sort_values('Avg Score', ascending=False)\n",
    "display(df_configs.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot alpha sensitivity for Linear fusion\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for fusion_idx, fusion in enumerate(fusion_methods):\n",
    "    alphas = []\n",
    "    avg_providers_list = []\n",
    "    avg_scores_list = []\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        key = f'{fusion}_alpha_{alpha}'\n",
    "        if key in config_scores:\n",
    "            alphas.append(alpha)\n",
    "            avg_providers_list.append(np.mean(config_scores[key]['n_providers']))\n",
    "            avg_scores_list.append(np.mean(config_scores[key]['avg_score']))\n",
    "    \n",
    "    ax = axes[fusion_idx]\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    line1 = ax.plot(alphas, avg_scores_list, 'o-', color='#2196F3', label='Avg Score', linewidth=2)\n",
    "    line2 = ax2.plot(alphas, avg_providers_list, 's--', color='#4CAF50', label='Avg Providers', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Alpha (BM25 weight)')\n",
    "    ax.set_ylabel('Average Score', color='#2196F3')\n",
    "    ax2.set_ylabel('Avg Unique Providers', color='#4CAF50')\n",
    "    ax.set_title(f'{fusion.upper()} Fusion')\n",
    "    \n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='best')\n",
    "\n",
    "plt.suptitle('Fusion Parameter Sensitivity Analysis', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/retrieval_fusion_sensitivity.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. System Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comprehensive comparison table\n",
    "summary_rows = []\n",
    "\n",
    "for sys_name in ['bm25', 'dense', 'hybrid_rrf']:\n",
    "    times = []\n",
    "    n_providers = []\n",
    "    \n",
    "    for query in test_queries:\n",
    "        data = all_results[query][sys_name]\n",
    "        times.append(data['time'] * 1000)\n",
    "        providers = set(r.cloud_provider for r in data['results'])\n",
    "        n_providers.append(len(providers))\n",
    "    \n",
    "    summary_rows.append({\n",
    "        'System': sys_name.replace('_', ' ').upper(),\n",
    "        'Avg Time (ms)': round(np.mean(times), 1),\n",
    "        'Std Time (ms)': round(np.std(times), 1),\n",
    "        'Avg Providers': round(np.mean(n_providers), 2),\n",
    "        'Max Providers': max(n_providers),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows).set_index('System')\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "systems = df_summary.index.tolist()\n",
    "colors = ['#FF9800', '#2196F3', '#4CAF50']\n",
    "\n",
    "# Time\n",
    "axes[0].bar(systems, df_summary['Avg Time (ms)'], color=colors, alpha=0.8)\n",
    "axes[0].set_ylabel('Average Time (ms)')\n",
    "axes[0].set_title('Response Time')\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# Provider diversity\n",
    "axes[1].bar(systems, df_summary['Avg Providers'], color=colors, alpha=0.8)\n",
    "axes[1].set_ylabel('Avg Unique Providers in Top-5')\n",
    "axes[1].set_title('Provider Diversity')\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.suptitle('Retrieval System Comparison', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/retrieval_comparison_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('RETRIEVAL SYSTEM COMPARISON - KEY FINDINGS')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('1. BM25 (Control 1): Fast lexical matching, good for exact keyword queries')\n",
    "print('   - Best for: Single-provider, specific technical terms')\n",
    "print('   - Limitation: Cannot capture semantic similarity')\n",
    "print()\n",
    "print('2. Dense (Control 2): Semantic understanding, captures meaning')\n",
    "print('   - Best for: Conceptual queries, paraphrased questions')\n",
    "print('   - Limitation: Can miss exact keyword matches')\n",
    "print()\n",
    "print('3. Hybrid (Experimental): Combines both strengths')\n",
    "print('   - Best for: Cross-cloud queries, complex questions')\n",
    "print('   - Advantage: Higher provider diversity in results')\n",
    "print('   - Trade-off: Slightly higher latency')\n",
    "print()\n",
    "print('Next steps: Evaluate with ground truth labels (Phase 3)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
