{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Embedding Model Comparison\n",
    "\n",
    "**Tesis:** Dise\u00f1o y Validaci\u00f3n de un Modelo Sem\u00e1ntico H\u00edbrido para Optimizar Sistemas RAG\n",
    "\n",
    "This notebook compares 4 embedding models on our cloud documentation corpus:\n",
    "1. **all-MiniLM-L6-v2** (384d) - Lightweight baseline\n",
    "2. **BGE-large-en-v1.5** (1024d) - State-of-the-art (primary candidate)\n",
    "3. **E5-large-v2** (1024d) - Competitive alternative\n",
    "4. **Instructor-large** (768d) - Task-specific instructions\n",
    "\n",
    "**Metrics:** Embedding time, file size, VRAM usage, t-SNE visualization, cross-provider similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Sample Chunks\n",
    "\n",
    "We'll use a representative sample of adaptive/500 chunks for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all adaptive/500 chunks\n",
    "chunks_dir = Path('../data/chunks/adaptive/size_500')\n",
    "all_chunks = []\n",
    "\n",
    "for jf in sorted(chunks_dir.glob('*.json')):\n",
    "    data = json.loads(jf.read_text(encoding='utf-8'))\n",
    "    all_chunks.append(data)\n",
    "\n",
    "print(f'Total chunks: {len(all_chunks)}')\n",
    "\n",
    "# Provider distribution\n",
    "providers = {}\n",
    "for c in all_chunks:\n",
    "    p = c.get('cloud_provider', 'unknown')\n",
    "    providers[p] = providers.get(p, 0) + 1\n",
    "\n",
    "for p, count in sorted(providers.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'  {p}: {count} chunks ({count/len(all_chunks)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a stratified sample (500 per provider, max 2000 total) for faster comparison\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "SAMPLE_SIZE_PER_PROVIDER = 400\n",
    "\n",
    "by_provider = {}\n",
    "for c in all_chunks:\n",
    "    p = c.get('cloud_provider', 'unknown')\n",
    "    by_provider.setdefault(p, []).append(c)\n",
    "\n",
    "sample_chunks = []\n",
    "for p, chunks_list in by_provider.items():\n",
    "    n = min(SAMPLE_SIZE_PER_PROVIDER, len(chunks_list))\n",
    "    sample_chunks.extend(random.sample(chunks_list, n))\n",
    "\n",
    "random.shuffle(sample_chunks)\n",
    "sample_texts = [c['text'] for c in sample_chunks]\n",
    "sample_providers = [c.get('cloud_provider', 'unknown') for c in sample_chunks]\n",
    "sample_ids = [c['chunk_id'] for c in sample_chunks]\n",
    "\n",
    "print(f'Sample size: {len(sample_chunks)}')\n",
    "for p in sorted(set(sample_providers)):\n",
    "    count = sample_providers.count(p)\n",
    "    print(f'  {p}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embed with All 4 Models\n",
    "\n",
    "We measure: embedding time, throughput (docs/sec), VRAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.embedding.embedding_manager import EmbeddingManager, MODEL_CONFIGS\n",
    "\n",
    "models_to_test = ['all-MiniLM-L6-v2', 'bge-large', 'e5-large']\n",
    "\n",
    "# Add instructor-large only if InstructorEmbedding is installed\n",
    "try:\n",
    "    import InstructorEmbedding\n",
    "    models_to_test.append('instructor-large')\n",
    "    print('InstructorEmbedding available - testing 4 models')\n",
    "except ImportError:\n",
    "    print('InstructorEmbedding not installed - testing 3 models (instructor-large will use SentenceTransformer fallback)')\n",
    "    models_to_test.append('instructor-large')  # Will use fallback\n",
    "\n",
    "print(f'\\nModels to test: {models_to_test}')\n",
    "print(f'Device: {\"cuda\" if torch.cuda.is_available() else \"cpu\"}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed with each model and collect metrics\n",
    "results = {}\n",
    "\n",
    "for model_name in models_to_test:\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Testing: {model_name}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        vram_before = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    mgr = EmbeddingManager(\n",
    "        model_name=model_name,\n",
    "        cache_dir='../data/embeddings',\n",
    "        batch_size=64,\n",
    "    )\n",
    "    \n",
    "    # Force model load\n",
    "    _ = mgr.model\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        vram_model = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    # Embed\n",
    "    start = time.time()\n",
    "    embeddings = mgr.embed_documents(sample_texts, show_progress=True)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        vram_peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "    else:\n",
    "        vram_model = 0\n",
    "        vram_peak = 0\n",
    "    \n",
    "    # Estimate file size\n",
    "    file_size_mb = embeddings.nbytes / (1024 * 1024)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'embeddings': embeddings,\n",
    "        'dimension': embeddings.shape[1],\n",
    "        'time_seconds': elapsed,\n",
    "        'throughput': len(sample_texts) / elapsed,\n",
    "        'vram_model_mb': vram_model,\n",
    "        'vram_peak_mb': vram_peak,\n",
    "        'file_size_mb': file_size_mb,\n",
    "    }\n",
    "    \n",
    "    print(f'  Dimension: {embeddings.shape[1]}')\n",
    "    print(f'  Time: {elapsed:.1f}s ({len(sample_texts)/elapsed:.0f} docs/sec)')\n",
    "    print(f'  VRAM (model): {vram_model:.0f} MB')\n",
    "    print(f'  VRAM (peak): {vram_peak:.0f} MB')\n",
    "    print(f'  File size ({len(sample_texts)} docs): {file_size_mb:.1f} MB')\n",
    "    \n",
    "    # Cleanup\n",
    "    del mgr\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison DataFrame\n",
    "rows = []\n",
    "for model_name, r in results.items():\n",
    "    config = MODEL_CONFIGS[model_name]\n",
    "    rows.append({\n",
    "        'Model': model_name,\n",
    "        'Full Name': config['full_name'],\n",
    "        'Dimension': r['dimension'],\n",
    "        'Time (s)': round(r['time_seconds'], 1),\n",
    "        'Throughput (docs/s)': round(r['throughput'], 0),\n",
    "        'VRAM Model (MB)': round(r['vram_model_mb'], 0),\n",
    "        'VRAM Peak (MB)': round(r['vram_peak_mb'], 0),\n",
    "        'File Size (MB)': round(r['file_size_mb'], 1),\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(rows)\n",
    "df_comparison.set_index('Model', inplace=True)\n",
    "display(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Time & Throughput\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "models = list(results.keys())\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0']\n",
    "\n",
    "# Time\n",
    "times = [results[m]['time_seconds'] for m in models]\n",
    "axes[0].barh(models, times, color=colors[:len(models)])\n",
    "axes[0].set_xlabel('Time (seconds)')\n",
    "axes[0].set_title('Embedding Time')\n",
    "for i, v in enumerate(times):\n",
    "    axes[0].text(v + 0.3, i, f'{v:.1f}s', va='center')\n",
    "\n",
    "# Throughput\n",
    "throughputs = [results[m]['throughput'] for m in models]\n",
    "axes[1].barh(models, throughputs, color=colors[:len(models)])\n",
    "axes[1].set_xlabel('Documents / second')\n",
    "axes[1].set_title('Throughput')\n",
    "for i, v in enumerate(throughputs):\n",
    "    axes[1].text(v + 1, i, f'{v:.0f}', va='center')\n",
    "\n",
    "# VRAM\n",
    "vrams = [results[m]['vram_peak_mb'] for m in models]\n",
    "axes[2].barh(models, vrams, color=colors[:len(models)])\n",
    "axes[2].set_xlabel('Peak VRAM (MB)')\n",
    "axes[2].set_title('GPU Memory Usage')\n",
    "for i, v in enumerate(vrams):\n",
    "    axes[2].text(v + 10, i, f'{v:.0f} MB', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/embedding_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: output/figures/embedding_performance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. t-SNE Visualization\n",
    "\n",
    "Visualize how each model clusters documents by cloud provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE for each model (use subsample for speed)\n",
    "TSNE_SAMPLE = min(1000, len(sample_chunks))\n",
    "tsne_idx = random.sample(range(len(sample_chunks)), TSNE_SAMPLE)\n",
    "tsne_providers = [sample_providers[i] for i in tsne_idx]\n",
    "\n",
    "provider_colors = {\n",
    "    'aws': '#FF9900',\n",
    "    'azure': '#0078D4',\n",
    "    'gcp': '#4285F4',\n",
    "    'kubernetes': '#326CE5',\n",
    "    'cncf': '#00B39F',\n",
    "}\n",
    "\n",
    "n_models = len(models)\n",
    "fig, axes = plt.subplots(1, n_models, figsize=(6 * n_models, 5))\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax_idx, model_name in enumerate(models):\n",
    "    emb_sub = results[model_name]['embeddings'][tsne_idx]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "    coords = tsne.fit_transform(emb_sub)\n",
    "    \n",
    "    for provider in sorted(set(tsne_providers)):\n",
    "        mask = [p == provider for p in tsne_providers]\n",
    "        color = provider_colors.get(provider, '#999999')\n",
    "        axes[ax_idx].scatter(\n",
    "            coords[mask, 0], coords[mask, 1],\n",
    "            c=color, label=provider, alpha=0.5, s=10,\n",
    "        )\n",
    "    \n",
    "    axes[ax_idx].set_title(f'{model_name}\\n(dim={results[model_name][\"dimension\"]})', fontsize=11)\n",
    "    axes[ax_idx].set_xticks([])\n",
    "    axes[ax_idx].set_yticks([])\n",
    "    if ax_idx == 0:\n",
    "        axes[ax_idx].legend(fontsize=8, loc='lower left')\n",
    "\n",
    "plt.suptitle('t-SNE: Embedding Space by Cloud Provider', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/embedding_tsne.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Saved: output/figures/embedding_tsne.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Provider Similarity Analysis\n",
    "\n",
    "How similar are equivalent services across providers? (e.g., AWS Lambda vs Azure Functions vs GCP Cloud Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-provider equivalent queries\n",
    "cross_provider_queries = [\n",
    "    'How to create a virtual machine instance',\n",
    "    'Serverless function deployment and configuration',\n",
    "    'Object storage bucket creation and management',\n",
    "    'Container orchestration with Kubernetes',\n",
    "    'Identity and access management IAM policies',\n",
    "    'Virtual private cloud VPC networking setup',\n",
    "    'Load balancer configuration and health checks',\n",
    "    'Database managed service setup and scaling',\n",
    "]\n",
    "\n",
    "print(f'Testing {len(cross_provider_queries)} cross-provider queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, embed queries and compute similarity to provider-specific chunks\n",
    "cross_provider_results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    print(f'\\nAnalyzing: {model_name}')\n",
    "    mgr = EmbeddingManager(model_name=model_name, cache_dir='../data/embeddings', batch_size=64)\n",
    "    \n",
    "    # Embed queries\n",
    "    query_embs = []\n",
    "    for q in cross_provider_queries:\n",
    "        query_embs.append(mgr.embed_query(q))\n",
    "    query_embs = np.array(query_embs)\n",
    "    \n",
    "    # Compute similarity per provider\n",
    "    provider_sims = {}\n",
    "    for provider in sorted(set(sample_providers)):\n",
    "        mask = [p == provider for p in sample_providers]\n",
    "        provider_embs = results[model_name]['embeddings'][mask]\n",
    "        \n",
    "        # Average max similarity across queries\n",
    "        sims = cosine_similarity(query_embs, provider_embs)\n",
    "        avg_max_sim = np.mean(np.max(sims, axis=1))\n",
    "        provider_sims[provider] = avg_max_sim\n",
    "    \n",
    "    cross_provider_results[model_name] = provider_sims\n",
    "    \n",
    "    for p, s in sorted(provider_sims.items()):\n",
    "        print(f'  {p}: avg max sim = {s:.4f}')\n",
    "    \n",
    "    del mgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Cross-provider similarity per model\n",
    "fig, axes = plt.subplots(1, len(models), figsize=(5 * len(models), 4))\n",
    "if len(models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax_idx, model_name in enumerate(models):\n",
    "    sims = cross_provider_results[model_name]\n",
    "    data = pd.DataFrame([sims]).T\n",
    "    data.columns = ['Avg Max Sim']\n",
    "    \n",
    "    sns.heatmap(\n",
    "        data, annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "        vmin=0.3, vmax=1.0, ax=axes[ax_idx]\n",
    "    )\n",
    "    axes[ax_idx].set_title(model_name, fontsize=11)\n",
    "\n",
    "plt.suptitle('Cross-Provider Query Similarity by Model', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/embedding_cross_provider_sim.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Intra-Cluster vs Inter-Cluster Similarity\n",
    "\n",
    "Measure how well each model separates provider-specific content from cross-provider content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average intra/inter-provider cosine similarity\n",
    "CLUSTER_SAMPLE = min(200, len(sample_chunks))\n",
    "cluster_idx = random.sample(range(len(sample_chunks)), CLUSTER_SAMPLE)\n",
    "cluster_providers = [sample_providers[i] for i in cluster_idx]\n",
    "\n",
    "cluster_metrics = {}\n",
    "for model_name in models:\n",
    "    emb_sub = results[model_name]['embeddings'][cluster_idx]\n",
    "    sim_matrix = cosine_similarity(emb_sub)\n",
    "    \n",
    "    intra_sims = []\n",
    "    inter_sims = []\n",
    "    \n",
    "    for i in range(len(cluster_idx)):\n",
    "        for j in range(i + 1, len(cluster_idx)):\n",
    "            if cluster_providers[i] == cluster_providers[j]:\n",
    "                intra_sims.append(sim_matrix[i, j])\n",
    "            else:\n",
    "                inter_sims.append(sim_matrix[i, j])\n",
    "    \n",
    "    cluster_metrics[model_name] = {\n",
    "        'intra_mean': np.mean(intra_sims),\n",
    "        'intra_std': np.std(intra_sims),\n",
    "        'inter_mean': np.mean(inter_sims),\n",
    "        'inter_std': np.std(inter_sims),\n",
    "        'separation': np.mean(intra_sims) - np.mean(inter_sims),\n",
    "    }\n",
    "\n",
    "df_cluster = pd.DataFrame(cluster_metrics).T\n",
    "df_cluster.columns = ['Intra-Provider Mean', 'Intra-Provider Std', 'Inter-Provider Mean', 'Inter-Provider Std', 'Separation']\n",
    "display(df_cluster.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Intra vs Inter similarity\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "intra = [cluster_metrics[m]['intra_mean'] for m in models]\n",
    "inter = [cluster_metrics[m]['inter_mean'] for m in models]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, intra, width, label='Intra-Provider', color='#4CAF50', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, inter, width, label='Inter-Provider', color='#FF5722', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Average Cosine Similarity')\n",
    "ax.set_title('Intra-Provider vs Inter-Provider Similarity')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=15)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add separation score annotations\n",
    "for i, m in enumerate(models):\n",
    "    sep = cluster_metrics[m]['separation']\n",
    "    ax.annotate(\n",
    "        f'\\u0394={sep:.3f}', xy=(i, max(intra[i], inter[i]) + 0.02),\n",
    "        ha='center', fontsize=9, color='navy', fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../output/figures/embedding_cluster_separation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Corpus Embedding (with selected model)\n",
    "\n",
    "Embed the full adaptive/500 corpus with the primary model (BGE-large) for use in retrieval experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if full embeddings already cached\n",
    "from src.embedding.embedding_manager import EmbeddingManager\n",
    "\n",
    "primary_model = 'bge-large'\n",
    "mgr = EmbeddingManager(model_name=primary_model, cache_dir='../data/embeddings', batch_size=64)\n",
    "\n",
    "cached = mgr.load_embeddings('adaptive', 500)\n",
    "if cached is not None:\n",
    "    print(f'Full embeddings already cached: {cached[0].shape}')\n",
    "    print(f'Chunk IDs: {len(cached[1])}')\n",
    "else:\n",
    "    print('Full embeddings not cached yet.')\n",
    "    print('Run: python scripts/build_index.py --embedding bge-large --chunker adaptive --size 500')\n",
    "    print('Or run the next cell to embed now.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: Embed full corpus here (will take several minutes on GPU)\n",
    "if cached is None:\n",
    "    all_texts = [c['text'] for c in all_chunks]\n",
    "    all_ids = [c['chunk_id'] for c in all_chunks]\n",
    "    \n",
    "    print(f'Embedding {len(all_texts)} chunks with {primary_model}...')\n",
    "    start = time.time()\n",
    "    embeddings, ids = mgr.embed_and_cache(all_texts, all_ids, 'adaptive', 500)\n",
    "    elapsed = time.time() - start\n",
    "    print(f'Done in {elapsed:.1f}s ({len(all_texts)/elapsed:.0f} docs/sec)')\n",
    "    print(f'Embeddings shape: {embeddings.shape}')\n",
    "else:\n",
    "    print('Using cached embeddings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Key findings from embedding model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('EMBEDDING MODEL COMPARISON - SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "# Identify best model for each metric\n",
    "fastest = min(results, key=lambda m: results[m]['time_seconds'])\n",
    "smallest = min(results, key=lambda m: results[m]['file_size_mb'])\n",
    "best_sep = max(cluster_metrics, key=lambda m: cluster_metrics[m]['separation'])\n",
    "least_vram = min(results, key=lambda m: results[m]['vram_peak_mb'])\n",
    "\n",
    "print(f'\\nFastest model:         {fastest} ({results[fastest][\"time_seconds\"]:.1f}s)')\n",
    "print(f'Smallest file size:    {smallest} ({results[smallest][\"file_size_mb\"]:.1f} MB)')\n",
    "print(f'Best cluster sep.:     {best_sep} (delta={cluster_metrics[best_sep][\"separation\"]:.4f})')\n",
    "print(f'Least VRAM:            {least_vram} ({results[least_vram][\"vram_peak_mb\"]:.0f} MB)')\n",
    "\n",
    "print(f'\\nRecommendation: Use {primary_model} as primary embedding model for thesis experiments.')\n",
    "print('Rationale: Best balance of quality (MTEB benchmarks) and cloud-domain performance.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
