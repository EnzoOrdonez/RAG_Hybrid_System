{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Exploration\n",
    "\n",
    "**Thesis:** Diseno y Validacion de un Modelo Semantico Hibrido para Optimizar Sistemas RAG\n",
    "\n",
    "This notebook explores the corpus, terminology, chunking strategies, and deduplication results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Publication-quality settings\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'figure.dpi': 300,\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "})\n",
    "sns.set_theme(style='whitegrid', palette='Set2')\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "CHUNKS_DIR = PROJECT_ROOT / 'data' / 'chunks'\n",
    "FIGURES_DIR = PROJECT_ROOT / 'output' / 'figures'\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all processed documents\n",
    "docs = []\n",
    "for json_file in PROCESSED_DIR.rglob('*.json'):\n",
    "    try:\n",
    "        data = json.loads(json_file.read_text(encoding='utf-8'))\n",
    "        docs.append(data)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(docs)\n",
    "print(f'Total documents loaded: {len(df)}')\n",
    "if len(df) > 0:\n",
    "    print(f'Columns: {list(df.columns)}')\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents per provider\n",
    "if len(df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count by provider\n",
    "    provider_counts = df['cloud_provider'].value_counts()\n",
    "    provider_counts.plot(kind='bar', ax=axes[0], color=sns.color_palette('Set2'))\n",
    "    axes[0].set_title('Documents per Provider')\n",
    "    axes[0].set_xlabel('Provider')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Word count by provider\n",
    "    word_totals = df.groupby('cloud_provider')['word_count'].sum()\n",
    "    word_totals.plot(kind='bar', ax=axes[1], color=sns.color_palette('Set2'))\n",
    "    axes[1].set_title('Total Words per Provider')\n",
    "    axes[1].set_xlabel('Provider')\n",
    "    axes[1].set_ylabel('Words')\n",
    "    axes[1].tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'corpus_by_provider.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No documents found. Run the download pipeline first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document size distribution\n",
    "if len(df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    df['word_count'].hist(bins=50, ax=axes[0], color='steelblue', edgecolor='white')\n",
    "    axes[0].set_title('Document Size Distribution (Words)')\n",
    "    axes[0].set_xlabel('Word Count')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].axvline(df['word_count'].median(), color='red', linestyle='--', label=f'Median: {df[\"word_count\"].median():.0f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Doc type distribution\n",
    "    if 'doc_type' in df.columns:\n",
    "        df['doc_type'].value_counts().plot(kind='bar', ax=axes[1], color=sns.color_palette('Set2'))\n",
    "        axes[1].set_title('Document Types')\n",
    "        axes[1].set_xlabel('Type')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'document_size_distribution.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Terminology Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze detected terminology\n",
    "if len(df) > 0 and 'terminology' in df.columns:\n",
    "    all_siglas = []\n",
    "    all_terms = []\n",
    "    for _, row in df.iterrows():\n",
    "        term_data = row.get('terminology', {})\n",
    "        if isinstance(term_data, dict):\n",
    "            all_siglas.extend(term_data.get('detected_siglas', []))\n",
    "            all_terms.extend(term_data.get('normalized_terms', []))\n",
    "    \n",
    "    if all_siglas:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Top 30 acronyms\n",
    "        sigla_counts = Counter(all_siglas).most_common(30)\n",
    "        if sigla_counts:\n",
    "            names, counts = zip(*sigla_counts)\n",
    "            axes[0].barh(range(len(names)), counts, color='steelblue')\n",
    "            axes[0].set_yticks(range(len(names)))\n",
    "            axes[0].set_yticklabels(names)\n",
    "            axes[0].set_title('Top 30 Acronyms in Corpus')\n",
    "            axes[0].set_xlabel('Frequency')\n",
    "            axes[0].invert_yaxis()\n",
    "        \n",
    "        # Top 20 normalized concepts\n",
    "        term_counts = Counter(all_terms).most_common(20)\n",
    "        if term_counts:\n",
    "            names, counts = zip(*term_counts)\n",
    "            axes[1].barh(range(len(names)), counts, color='coral')\n",
    "            axes[1].set_yticks(range(len(names)))\n",
    "            axes[1].set_yticklabels(names)\n",
    "            axes[1].set_title('Top 20 Normalized Concepts')\n",
    "            axes[1].set_xlabel('Frequency')\n",
    "            axes[1].invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'terminology_analysis.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No terminology data found. Run terminology normalizer first.')\n",
    "else:\n",
    "    print('No terminology data available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-provider term heatmap\n",
    "if len(df) > 0 and 'terminology' in df.columns:\n",
    "    provider_terms = defaultdict(set)\n",
    "    for _, row in df.iterrows():\n",
    "        prov = row.get('cloud_provider', '')\n",
    "        term_data = row.get('terminology', {})\n",
    "        if isinstance(term_data, dict):\n",
    "            for t in term_data.get('normalized_terms', []):\n",
    "                provider_terms[prov].add(t)\n",
    "    \n",
    "    if provider_terms:\n",
    "        providers = sorted(provider_terms.keys())\n",
    "        # Compute shared term counts\n",
    "        matrix = []\n",
    "        for p1 in providers:\n",
    "            row = []\n",
    "            for p2 in providers:\n",
    "                shared = len(provider_terms[p1] & provider_terms[p2])\n",
    "                row.append(shared)\n",
    "            matrix.append(row)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        sns.heatmap(\n",
    "            matrix, annot=True, fmt='d',\n",
    "            xticklabels=[p.upper() for p in providers],\n",
    "            yticklabels=[p.upper() for p in providers],\n",
    "            cmap='YlOrRd', ax=ax\n",
    "        )\n",
    "        ax.set_title('Shared Normalized Terms Between Providers')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'cross_provider_terms_heatmap.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunking Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks from all strategies\n",
    "chunk_data = []\n",
    "for strategy_dir in sorted(CHUNKS_DIR.iterdir()) if CHUNKS_DIR.exists() else []:\n",
    "    if not strategy_dir.is_dir():\n",
    "        continue\n",
    "    strategy = strategy_dir.name\n",
    "    for size_dir in sorted(strategy_dir.iterdir()):\n",
    "        if not size_dir.is_dir():\n",
    "            continue\n",
    "        size = size_dir.name.replace('size_', '')\n",
    "        count = 0\n",
    "        token_counts = []\n",
    "        code_chunks = 0\n",
    "        table_chunks = 0\n",
    "        for jf in size_dir.glob('*.json'):\n",
    "            try:\n",
    "                data = json.loads(jf.read_text(encoding='utf-8'))\n",
    "                count += 1\n",
    "                token_counts.append(data.get('token_count', 0))\n",
    "                if data.get('has_code'):\n",
    "                    code_chunks += 1\n",
    "                if data.get('has_table'):\n",
    "                    table_chunks += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        if count > 0:\n",
    "            chunk_data.append({\n",
    "                'strategy': strategy,\n",
    "                'size': int(size),\n",
    "                'num_chunks': count,\n",
    "                'avg_tokens': np.mean(token_counts) if token_counts else 0,\n",
    "                'std_tokens': np.std(token_counts) if token_counts else 0,\n",
    "                'min_tokens': min(token_counts) if token_counts else 0,\n",
    "                'max_tokens': max(token_counts) if token_counts else 0,\n",
    "                'pct_code': code_chunks / count * 100,\n",
    "                'pct_table': table_chunks / count * 100,\n",
    "            })\n",
    "\n",
    "if chunk_data:\n",
    "    chunk_df = pd.DataFrame(chunk_data)\n",
    "    print(chunk_df.to_string(index=False))\n",
    "else:\n",
    "    print('No chunks found. Run chunking first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk count comparison\n",
    "if chunk_data:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Chunk count by strategy and size\n",
    "    pivot = chunk_df.pivot(index='strategy', columns='size', values='num_chunks')\n",
    "    pivot.plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Number of Chunks by Strategy and Size')\n",
    "    axes[0].set_xlabel('Strategy')\n",
    "    axes[0].set_ylabel('Number of Chunks')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].legend(title='Chunk Size')\n",
    "    \n",
    "    # Average token count\n",
    "    pivot_avg = chunk_df.pivot(index='strategy', columns='size', values='avg_tokens')\n",
    "    pivot_avg.plot(kind='bar', ax=axes[1])\n",
    "    axes[1].set_title('Average Token Count per Chunk')\n",
    "    axes[1].set_xlabel('Strategy')\n",
    "    axes[1].set_ylabel('Avg Tokens')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].legend(title='Chunk Size')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'chunking_comparison.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token distribution per strategy (box plot)\n",
    "if CHUNKS_DIR.exists():\n",
    "    all_chunk_tokens = []\n",
    "    target_size = '500'\n",
    "    \n",
    "    for strategy_dir in sorted(CHUNKS_DIR.iterdir()):\n",
    "        if not strategy_dir.is_dir():\n",
    "            continue\n",
    "        size_dir = strategy_dir / f'size_{target_size}'\n",
    "        if not size_dir.exists():\n",
    "            continue\n",
    "        for jf in list(size_dir.glob('*.json'))[:500]:  # Sample for speed\n",
    "            try:\n",
    "                data = json.loads(jf.read_text(encoding='utf-8'))\n",
    "                all_chunk_tokens.append({\n",
    "                    'strategy': strategy_dir.name,\n",
    "                    'tokens': data.get('token_count', 0),\n",
    "                })\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    if all_chunk_tokens:\n",
    "        token_df = pd.DataFrame(all_chunk_tokens)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.boxplot(data=token_df, x='strategy', y='tokens', ax=ax)\n",
    "        ax.axhline(y=int(target_size), color='red', linestyle='--', alpha=0.7, label=f'Target: {target_size}')\n",
    "        ax.set_title(f'Token Distribution per Chunk (Target Size: {target_size})')\n",
    "        ax.set_xlabel('Strategy')\n",
    "        ax.set_ylabel('Token Count')\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'chunk_token_distribution.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deduplication Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dedup stats\n",
    "dedup_path = PROJECT_ROOT / 'data' / 'dedup_stats.json'\n",
    "if dedup_path.exists():\n",
    "    dedup_stats = json.loads(dedup_path.read_text(encoding='utf-8'))\n",
    "    \n",
    "    print('=== Deduplication Results ===')\n",
    "    print(f'Input documents:  {dedup_stats.get(\"input_documents\", \"N/A\")}')\n",
    "    print(f'Output documents: {dedup_stats.get(\"output_documents\", \"N/A\")}')\n",
    "    print(f'\\nIntra-document paragraphs removed: {dedup_stats.get(\"intra_document\", {}).get(\"paragraphs_removed\", 0)}')\n",
    "    print(f'Intra-provider duplicates: {dedup_stats.get(\"intra_provider\", {}).get(\"duplicates_found\", 0)}')\n",
    "    print(f'Cross-provider equivalent groups: {dedup_stats.get(\"cross_provider\", {}).get(\"equivalents_marked\", 0)}')\n",
    "else:\n",
    "    print('No deduplication stats found. Run deduplicator first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "if len(df) > 0:\n",
    "    summary = df.groupby('cloud_provider').agg({\n",
    "        'doc_id': 'count',\n",
    "        'word_count': ['sum', 'mean', 'median'],\n",
    "        'has_code': 'sum',\n",
    "        'has_tables': 'sum',\n",
    "    }).round(0)\n",
    "    summary.columns = ['Documents', 'Total Words', 'Avg Words', 'Median Words', 'With Code', 'With Tables']\n",
    "    print('\\n=== Corpus Summary ===')\n",
    "    print(summary.to_string())\n",
    "    \n",
    "    # Save as CSV for thesis\n",
    "    summary.to_csv(PROJECT_ROOT / 'output' / 'tables' / 'corpus_summary.csv')\n",
    "    print(f'\\nSaved to output/tables/corpus_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
